# ğŸ›¡ï¸ WARDEN : Credit Card Fraud Detector 
Presenting âœ¨ the almighty **WARDEN** ğŸ›¡ï¸. Warden is a state-of-the-art End to End MLOps project designed for credit card fraud detection. Leveraging advance machine learning algorithms, MLOps best practices. Warden incorporates Alerting and monitoring system with robust model development and deployment. 

With its intuitive Streamlit interface, Warden offers both batch and real-time inference, accompanied by comprehensive reports. Built-in monitoring guarantees continuous data quality assessment, making Warden a formidable guardian against fraudulent transactions.

This project aims to streamline the process of filtering the fraud transactions, providing insights, comprehensive reports and predictions to drive impactful business decisions! ğŸ“Šâœ¨


# Table of Contents ğŸ“‘
+ [Problem Statement](#problem-statement-) â“
+ [Solution Approach](#solution-approach-) ğŸ¯
+ [Folder Structure](#folder-structure-) ğŸ“
+ [Project Overview](#project-overview-) ğŸ‘ï¸
+ [Prerequisites](#prerequisites-) ğŸ“‹
+ [Running the project](#running-the-project-) ğŸƒâ€â™‚ï¸


# Problem Statement â“
In 2022, U.S. credit card transactions totaled 54.8 billion for an average of 150.15 million per day, 6.25 million per hour, 104,274 per minute. or 1,739 per second. 
The firm estimated the global credit card transaction to be 21,510 per second. [link to the report ](https://capitaloneshopping.com/research/number-of-credit-card-transactions/)

As the information suggests ğŸ¤” , It is  impossible for a human to check each and every transaction let alone detect  ğŸ” which one is fraud or legitimate . . . 



# Solution Approach ğŸ¯
1. Researchers have found some common trends and patterns when a transaction is fraud suggesting the solution to be **Statistical Analysis**

2. But the amount of data still daunts us, 21k+ transactions per second is a mammoth data and can't be handled and analysed by statisticians alone. we need machine to do the work for us which leads us to **Artificial intelligence**


Upon reviewing the statements, its clear that the solution of the problem lies in **Machine learning** which leverages both statistical algorithms and artificial intelligence reducing human work load, cost and time.


The project proposes : an **E2E(End-to-End) ML solution** in which User can input details of transactions and recieve insight about the transaction whether its fraud or legit based on trends and patterns.



# Project Overview ğŸ‘ï¸

Our project consists of several stages:

## Data Collection ğŸ“
Due to its sensitive nature, Real world Credit card transaction data is often difficult to get hands on due to monetary and confidentiality issues, therefore we have used a [kaggle dataset](https://www.kaggle.com/datasets/kartik2112/fraud-detection) which is "synthetically generated".

## Data Handling ğŸ’¾
We used ORM (Object Relational Mapping) to handle the data. We shifted the data to a Postgresql server and fetched it from the database using ORM. Please find ORM steps and code in `data/management` folder.

## Exploratory Data Analysis ğŸ”
In EDA, we perfomed initial exploration of the data, familiarity with the features, Inspected the nature of the data through visualizations and various other statistical techniques such as Cross validation. We also noted down the necessary data pipeline steps we need to take for E2E pipeline. Please make sure to check our comprehensive [EDA Notebook](./notebooks/EDA.ipynb)

## ETL Data Pipeline ğŸšš
After initial exploration and identifying features that needed to be dealt with. We moved towards ETL in which we developed a ETL data pipeline involving necessary encodings and engineering to features and balancing the data through which we made the data into a format suitable for machine learning. Please find the necessary ETL steps in `steps/ETL` folder.

## Data Testing ğŸ“
Data testing is a part of monitoring system in WARDEN. Before passing the data to training, the data passes through data testing phase which conducts series of tests including : integrity checks, missing value handling, outlier detection, and feature distribution analysis. These tests can be aquired through dashboard anytime and are also alerted through slack. Please find the data testing code in `steps/monitoring/data_tester.py` file.

## Slack Alerts ğŸš¨
After the testing phase, the results are alerted to team through slackbot. Streamlining the process of alerting and monitoring. Please find the alerting code in `steps/monitoring/alerter.py` and `src/utils.py` files.

## Model Building ğŸ—ï¸
Through EDA, after testing through various algorithms we discovered that "Adaboost" performed better than other algorithm, therefore after data testing we trained our model on Adaptive boosting.

## Model Evaluation ğŸ“Š
Since its a classification problem and a problem in which False Negatives (Type II error) are more expensive than False Positives (Type I error) we paid special focus on Recall, ROC-AUC scores and F1 Score. Please find training and evaluation steps in `steps/training` folder.

## Model Deployment ğŸš€
We deployed model using BentoML and create a user-friendly web application using Streamlit. Users can input transaction details, and the model predicts wether the transaction is fraudualent or not. This application might help for pre-screening,enabling early identification of potential fraud transactions filtering them for further analysis. Please find the deployment steps in `steps/deployment` folder.

## Reports ğŸ“‘
Reports is another tool of Monitoring system. Whenever warden recieves a batch inference request,it auto-generates reports consisting of model performance decay, data drift, target drift, and many other useful metrics with beautiful visualizations. Please find the report code in `steps/monitoring/data_reporter.py` file.



# Folder structure ğŸ“

```
â”œâ”€â”€ assets ğŸ“
â”‚   â””â”€â”€ ...
â”œâ”€â”€ configs ğŸ“
â”‚   â”œâ”€â”€ deployment_config.py
â”‚   â””â”€â”€ etl_config.py
â”œâ”€â”€ data ğŸ“
â”‚   â”œâ”€â”€ fraud.csv
â”‚   â”œâ”€â”€ sample_batch_data.csv
â”‚   â””â”€â”€ management ğŸ“
â”‚       â”œâ”€â”€ fill_table.py
â”‚       â”œâ”€â”€ index.py
â”‚       â””â”€â”€ retriever.py
â”œâ”€â”€ materializer ğŸ“
â”‚   â””â”€â”€ custom_materializer.py
â”œâ”€â”€ notebooks ğŸ“
â”‚   â””â”€â”€ EDA.ipynb
â”œâ”€â”€ pipelines ğŸ“
â”‚   â”œâ”€â”€ deployment_pipeline.py
â”‚   â”œâ”€â”€ inference_pipeline.py
â”‚   â””â”€â”€ training_pipeline.py
â”œâ”€â”€ src ğŸ“
â”‚   â”œâ”€â”€ etl ğŸ“
â”‚   â”‚   â”œâ”€â”€ balance_data.py
â”‚   â”‚   â”œâ”€â”€ categorical_encoding.py
â”‚   â”‚   â”œâ”€â”€ data_ingestion.py
â”‚   â”‚   â””â”€â”€ feature_engineering.py
â”‚   â”œâ”€â”€ training ğŸ“
â”‚   â”‚   â”œâ”€â”€ model_building.py
â”‚   â”‚   â””â”€â”€ model_evaluation.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ steps ğŸ“
â”‚   â”œâ”€â”€ monitoring ğŸ“
â”‚   â”‚   â”œâ”€â”€ alerter.py
â”‚   â”‚   â”œâ”€â”€ data_reporter.py
â”‚   â”‚   â””â”€â”€ data_tester.py
â”‚   â”œâ”€â”€ deployment ğŸ“
â”‚   â”‚   â”œâ”€â”€ bento_builder.py
â”‚   â”‚   â”œâ”€â”€ deployment_trigger.py
â”‚   â”‚   â”œâ”€â”€ dynamic_importer.py
â”‚   â”‚   â”œâ”€â”€ model_loader.py
â”‚   â”‚   â”œâ”€â”€ prediction_service_loader.py
â”‚   â”‚   â””â”€â”€ predictor.py
â”‚   â”œâ”€â”€ etl ğŸ“
â”‚   â”‚   â”œâ”€â”€ categorical_encoder.py
â”‚   â”‚   â”œâ”€â”€ data_balancer.py
â”‚   â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”‚   â”œâ”€â”€ data_splitter.py
â”‚   â”‚   â””â”€â”€ feature_engineer.py
â”‚   â””â”€â”€ training ğŸ“
â”‚       â”œâ”€â”€ evaluate_model.py
â”‚       â””â”€â”€ train_model.py
â”œâ”€â”€ service.py
â”œâ”€â”€ streamlit_app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ run.py

```

# Features ğŸ¨
The important features and tech stack used in this project are as follows :-

## ORM (Object Relational Mapping) 
 Object Relational Mapping (ORM) simplifies database interactions by allowing us to work with database entities as Python objects. In this project, We used ORM to push data to and fetch data from a PostgreSQL server, abstracting away the need for direct SQL queries and making database operations more intuitive and manageable. ORM steps and code is in `data/management` folder.
 
 - After Running `data/management/index.py`
![ORM Table Creation](assets/ORM_table_creation.png)

 - After Running `data/management/fill_table.py`
![ORM Table Filled](assets/ORM_table_filled.png)


## ZenML
A comprehensive data science project often requires dealing with various industry tools and technologies for experiment tracking, testing, alerting, monitoring, deployment. These tools are useful but difficult to use without a centralized framework. 


ZenML is open source **MLOps framework** to 
streamline and automate the end-to-end machine learning lifecycle. Its flexible architecture allows for easy integration with popular tools and frameworks. ZenML provides a dashboard to keep track of pipelines, runs, artifacts, stack components and many more . . . 

- **Here is a glimps of ZenML dashboard**
![ZenML Dashboard](assets/zenml_runs.png)

- **ZenML also provides a beautiful DAGs to understand the flow**
![ZenML Dag](assets/dag.png)


- **The various popular tools integrated and streamlined with ZenML are here.**
![ZenML Dashboard](assets/zenml_stack.png)

## BentoML
To Deploy the project we used **BentoML Model Deployer** component. BentoML creates a self sufficient Bento with all necessary components in a unified Bento. This bento then can be deployed anywhere on cloud for predictions.

![BentoML deployer](assets/deploy.png)

## MLFlow
Every run in ML cycle is an experiment. We used MLFlow for **experiment tracking** in our project. MLFlow comes with its intuitive dashboard to track experiments, logs parameters, metrics and many more ...

![MLflow Dashboard](assets/mlflow_dashboard.png)


## EvidentlyAI
EvidentlyAI is a significant component in our project's **Model Monitoring system**. 

It helps in monitoring the performance and behavior of machine learning models in production, providing insights into model drift, data drift, and other relevant metrics for effective monitoring and management of deployed models.

We used Evidently for 2 purposes :
- Generating a comprehensive report after batch inference for various evaluations like model drift, data drift, classification performance with beautiful and interactive visualizations. Here is a [Sample Report](https://batch-report.netlify.app)

- Conducting tests on data before training the model on it. These tests are then alerted to slack channel through slackbot. Here is the [Sample Test Suite](https://test-suite.netlify.app)

## Slackbot Alerts
We integrated slackbot in our pipeline so that the tests results are notified to our team directly through slackbot in our channel. Slack alerters is another major part of our Alerting and Monitoring System.

![Slack Alerts](assets/slackbot.png)

# Running the project ğŸƒâ€â™‚ï¸

### Prerequisites ğŸ“‹
Before starting, ensure you have the following prerequisites:
- **Python 3.10**
- **Ubuntu**
- **VS Code (WSL)**
- **PostgreSQL server**
- **pgadmin4**

Please follow following steps to run the project on your machine locally . . . 

1. Clone this repository to your local machine.

    ```
    git clone https://github.com/Gaurav5416/WARDEN
    cd WARDEN
    ```

2. Install necessary requirements.
    ```
    pip install -r requirements.txt
    ```
3. Wake up zenml
    ```
    zenml init
    zenml up
    ```
4. Installing zenml integrations
    ```
    zenml integration install evidently -y
    zenml integration install bentoml -y
    zenml integration install mlflow -y
    zenml integration install slack -y
    ```

5. Registering necessary stack components
    ```
    zenml data-validator register evidently_validator --flavor=evidently
    zenml model-deployer register bentoml_deployer --flavor=bentoml
    zenml experiment-tracker register mlflow_tracker --flavor=mlflow
    zenml alerter register slack_alerter --flavor=slack --slack_token=<> --default_slack_channel_id=<>
    zenml stack register warden_stack -d bentoml_deployer -e mlflow_tracker -al slack_alerter -dv evidently_validator -o default -a default
    zenml stack set warden_stack
    ```

6. Create a .env file specifying datapath of table and database path eg..
    ```
    DB_URL = "postgresql://username:password@host:port/database"
    datapath = "WARDEN/data/fraud.csv"
    ```

7. Create SQL table and push CSV data into postgreSQL database using ORM
    ```
    cd data/management
    python index.py
    python fill_table.py
    ```

8. Deploying the ML Model
    ```
    python run.py
    ```

9. Running Streamlit app
    ```
    streamlit run streamlit_app.py
    ```

***